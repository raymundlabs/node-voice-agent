<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deepgram Voice Agent</title>
</head>
<body>
    <h1>Deepgram Voice Agent Browser Demo</h1>
    <p>This is a demo of the Deepgram Voice Agent. It uses the <a href="https://developers.deepgram.com/reference/voice-agent-api/agent">Deepgram Voice Agent API</a>.</p>
    <p>Please enable your microphone in the browser to start the conversation.</p>
    <script>
        let socket;
        let mediaStream;
        let audioContext;
        let processor;
        let isConnected = false;
        let audioQueue = []; // Queue for managing incoming audio chunks
        let isPlaying = false; // Flag to track if we're currently playing audio
        let selectedDeviceId;

        async function init() {
            try {
                // Create audio context early
                audioContext = new AudioContext({
                    sampleRate: 24000 // Match the highest sample rate we'll use
                });

                // Get microphone permission with specific constraints
                const constraints = {
                    audio: {
                        deviceId: selectedDeviceId ? { exact: selectedDeviceId } : undefined,
                        channelCount: 1,
                        sampleRate: 24000,
                        echoCancellation: false,  // Can be toggled
                        noiseSuppression: false,  // Can be toggled
                        autoGainControl: false,   // Can be toggled
                        latency: 0,              // Minimize latency
                        // Advanced constraints for better control
                        googEchoCancellation: false,
                        googAutoGainControl: false,
                        googNoiseSuppression: false,
                        googHighpassFilter: true
                    }
                };
                mediaStream = await navigator.mediaDevices.getUserMedia(constraints);

                // Connect to WebSocket server
                socket = new WebSocket(
                  (location.protocol === 'https:' ? 'wss://' : 'ws://') + location.host + '/'
                );

                socket.onopen = () => {
                    isConnected = true;
                    startStreaming();
                };

                socket.onmessage = async (event) => {
                    if (event.data instanceof Blob) {
                        try {
                            const arrayBuffer = await event.data.arrayBuffer();
                            const audioData = new Int16Array(arrayBuffer);
                            // Add to queue instead of playing immediately
                            audioQueue.push(audioData);
                            if (!isPlaying) {
                                playNextInQueue();
                            }
                        } catch (error) {
                            console.error('Error processing audio response:', error);
                        }
                    }
                };

                socket.onerror = (error) => {
                    console.error('WebSocket error:', error);
                    isConnected = false;
                };

                socket.onclose = () => {
                    isConnected = false;
                    stopStreaming();
                };
            } catch (error) {
                console.error('Error initializing:', error);
            }
        }

        async function setupAudioProcessing() {
            const source = audioContext.createMediaStreamSource(mediaStream);

            // Gain control
            const gainNode = audioContext.createGain();

            // Analyzer for volume monitoring
            const analyser = audioContext.createAnalyser();
            analyser.fftSize = 1024;

            // Worklet processor for better performance
            const processor = audioContext.createScriptProcessor(2048, 1, 1);

            // Connect the chain
            source
                .connect(gainNode)
                .connect(analyser)
                .connect(processor)
                .connect(audioContext.destination);

            return { gainNode, analyser, processor };
        }

        function startStreaming() {
            if (!mediaStream || !isConnected) return;

            try {
                const source = audioContext.createMediaStreamSource(mediaStream);

                // Create a worklet for better audio processing
                const bufferSize = 2048;
                processor = audioContext.createScriptProcessor(bufferSize, 1, 1);

                source.connect(processor);
                processor.connect(audioContext.destination);

                let lastSendTime = 0;
                const sendInterval = 100; // Send every 100ms

                processor.onaudioprocess = (e) => {
                    const now = Date.now();
                    if (socket?.readyState === WebSocket.OPEN && now - lastSendTime >= sendInterval) {
                        const inputData = e.inputBuffer.getChannelData(0);
                        const pcmData = convertFloatToPcm(inputData);
                        socket.send(pcmData.buffer);
                        lastSendTime = now;
                    }
                };
            } catch (error) {
                console.error('Error starting audio stream:', error);
            }
        }

        function convertFloatToPcm(floatData) {
            const pcmData = new Int16Array(floatData.length);
            for (let i = 0; i < floatData.length; i++) {
                const s = Math.max(-1, Math.min(1, floatData[i]));
                pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }
            return pcmData;
        }

        async function playNextInQueue() {
            if (audioQueue.length === 0) {
                isPlaying = false;
                return;
            }

            isPlaying = true;
            const audioData = audioQueue.shift();

            try {
                // Ensure audio context is running
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }

                // Create buffer with correct sample rate for agent's audio (24000Hz)
                const buffer = audioContext.createBuffer(1, audioData.length, 24000);
                const channelData = buffer.getChannelData(0);

                // Convert Int16 to Float32 with proper scaling
                for (let i = 0; i < audioData.length; i++) {
                    // Normalize to [-1, 1] range
                    channelData[i] = audioData[i] / (audioData[i] >= 0 ? 0x7FFF : 0x8000);
                }

                // Create and configure source
                const source = audioContext.createBufferSource();
                source.buffer = buffer;

                // Create a gain node for volume control
                const gainNode = audioContext.createGain();
                gainNode.gain.value = 1.0; // Adjust volume if needed

                // Connect nodes
                source.connect(gainNode);
                gainNode.connect(audioContext.destination);

                // Handle playback completion
                source.onended = () => {
                    playNextInQueue(); // Play next chunk when current one ends
                };

                // Start playback
                source.start(0);
            } catch (error) {
                console.error('Error playing audio:', error);
                isPlaying = false;
                playNextInQueue(); // Try next chunk if current fails
            }
        }

        function stopStreaming() {
            audioQueue = []; // Clear audio queue
            isPlaying = false;
            if (processor) {
                processor.disconnect();
                processor = null;
            }
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            isConnected = false;
        }

        function initializeVolumeMeter(analyser) {
            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);

            function updateMeter() {
                analyser.getByteFrequencyData(dataArray);
                const average = dataArray.reduce((a, b) => a + b) / bufferLength;
                const volume = Math.min(100, average * 2);
                // Update UI with volume level
                requestAnimationFrame(updateMeter);
            }

            updateMeter();
        }

        async function getAudioDevices() {
            const devices = await navigator.mediaDevices.enumerateDevices();
            return devices.filter(device => device.kind === 'audioinput');
        }

        // Initialize when the page loads
        window.onload = init;

        // Clean up when the page is closed
        window.onbeforeunload = () => {
            stopStreaming();
            if (socket) {
                socket.close();
            }
        };
    </script>
</body>
</html>
